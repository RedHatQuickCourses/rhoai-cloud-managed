= Section 1


=== AWS MarketPlace Product Overview - Red Hat OpenShift AI

Red Hat OpenShift AI enables companies to solve critical business challenges by providing a fully managed cloud service environment on Red Hat OpenShift Service on AWS. Red Hat OpenShift AI allows organizations to quickly build and deploy artificial intelligence (AI)/ML models by integrating open-source applications with commercial partner technology.


Red Hat OpenShift AI is an easy-to-configure cloud service that provides a powerful platform for building AI/ML models and applications. It combines the self-service data scientists and developers want with the confidence enterprise IT demands into one common platform. Common tooling, such as Jupyter notebooks and associated TensorFlow and Pytorch frameworks, are an add-on to Red Hat OpenShift Service on AWS, an application platform cloud service powered by Kubernetes and co-managed by Red Hat and Amazon.

Classic or HCP (hosted control planes) ROSA cluster.
Classic: I ran this tutorial on a classic single-AZ ROSA 4.15.17 cluster with m6a.4xlarge node with auto-scaling enabled up to 10 nodes. The cluster has 80 vCPUs with ~339Gi memory. 
HCP: I also ran this tutorial on an HCP ROSA 4.16.3 cluster with m5.xlarge node with 28 vCPUs and ~108Gi memory.


 rosa create machinepool --cluster=<your-cluster-name> --name=gpu-pool --instance-type=g5.4xlarge --min-replicas=1 --max-replicas=4 --enable-autoscaling --labels='gpu-node=true' --taints='nvidia.com/gpu=present:NoSchedule'



== Deploy and LLM on ROSA with HCP

Learn how to install the Red Hat® OpenShift® AI (RHOAI) operator and Jupyter notebook, create an Amazon S3 bucket, and run the LLM model on a Red Hat OpenShift Service on AWS (ROSA) cluster. https://cloud.redhat.com/learn/how-run-and-deploy-llms-using-red-hat-openshift-ai-red-hat-openshift-service-aws-cluster[LINK, window=blank]


https://console.redhat.com/openshift/create[Link to the Hybrid Cloud Console]


=== Rosa Cluster Deployment - CLI

Deploy ROSA cluster
set some environment variables


 Copy
  export ROSA_CLUSTER_NAME=mycluster
  export AWS_ACCOUNT_ID=`aws sts get-caller-identity \
    --query Account --output text`
  export REGION=us-east-2
  export AWS_PAGER=""

Make you your ROSA CLI version is correct (v1.2.25 or higher)


 Copy
  rosa version

Run the rosa cli to create your cluster

Note there are many configurable installation options that you can view using rosa create cluster -h. The following will create a cluster with all of the default options.


Copy
 rosa create cluster --sts --cluster-name ${ROSA_CLUSTER_NAME} \
   --region ${REGION} --mode auto --yes

Watch the install logs


Copy
 rosa logs install -c $ROSA_CLUSTER_NAME --watch --tail 10

Validate the cluster
Once the cluster has finished installing we can validate we can access it

Create an Admin user


Copy
 rosa create admin -c $ROSA_CLUSTER_NAME

Wait a few moments and run the oc login command it provides. If it fails, or if you get a warning about TLS certificates, wait a few minutes and try again.

Run oc whoami --show-console, browse to the provided URL and log in using the credentials provided above.

Cleanup
 Delete the ROSA cluster


 rosa delete cluster -c $ROSA_CLUSTER_NAME
 Clean up the STS roles

Once the cluster has been deleted we can delete the STS roles.

Tip You can get the correct commands with the ID filled in from the output of the previous step.



 rosa delete operator-roles -c <id> --yes --mode auto
 rosa delete oidc-provider -c <id>  --yes --mode auto


 === Install OpenShift AI

 From your cluster console, go to OperatorHub under Operators from the left tab, and put “Red Hat OpenShift AI” into the search query to install the operator. Note: The most recent version of the operator at the time of writing is 2.13.0.

Choose the default option for the installation. The operator will be installed in the redhat-ods-operator namespace, which will be created automatically upon installation.

Afterward, create a DataScienceCluster (DSC) instance. Once again, choose the default option for the installation. The name of the DSC instance will be default-dsc. 

=== Creating and granting access to Amazon S3 bucket

There are actually several ways to go about granting S3 access to the pods running in your Red Hat® OpenShift® Service on AWS (ROSA) cluster. For example, you can set the credentials as environment variables in the notebook using pod identity/Identity Access Management (IAM) Roles for Service Accounts (IRSA) to authenticate the pods to S3. Or, you can install the AWS command line interface (CLI) in the cluster, among others. 

For the sake of simplicity, we’ll install the CLI in the cluster and then use the command aws configure to provide the credentials. Be sure that you have your AWS access key and secret access key handy. You could create new keys in the IAM section from the AWS console if you have lost yours.  

SS CLI TO POD

Now log into your cluster and go to the namespace where your notebook is located:  
 oc project rhods-notebooks
Run the following command and make sure that your pods are running: oc get pods

Once you have the name of the pod (in this case it is called jupyter-nb-admin-0), execute into it: 
 oc exec -it jupyter-nb-admin-0 -- /bin/bash

Next, let's install the AWS CLI in that pod:

 curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
 unzip awscliv2.zip
 ./aws/install -i ~/.local/aws-cli -b ~/.local/bin

Then modify your PATH environment: 
 export PATH=~/.local/bin:$PATH

Once it is correctly installed, be sure that you have your AWS Access Key ID and AWS Secret Access Key ready, and run the following command: aws configure
Select the region where your cluster is located. You could verify the configuration by running simple command such as listing the S3 buckets: 
 aws s3 ls

Once the credentials matter is sorted, let's create an S3 bucket in your AWS account. Again, there are many ways to go about this. The easiest would be to go to your AWS console and create the bucket in your region from there and leave all the settings to default. 

Alternatively, you can run this command to create a bucket (in this case, it’s called llm-bucket-dsari, and the cluster region is us-west-2):
  aws s3 mb s3://llm-bucket-dsari --region us-west-2

Once these steps are complete, you’re ready to begin training the LLM model in the next resource. 




