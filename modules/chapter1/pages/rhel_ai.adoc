= RHEL AI on AWS

== Objectives

 * Explain how to deploy RHEL AI on AWS.
 * Optional: Hands on Activity - External Lab - Deploy Red Hat AI on AWS
 * Analyze the RHEL AI server requirements for Training versus Inference

== Excerpt from RHEL AI Documentation

Red Hat Enterprise Linux AI (RHEL AI) is a platform that allows you to develop enterprise applications on open source Large Language Models (LLMs). RHEL AI is built from the Red Hat InstructLab open source project. https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.2/html/getting_started/rhelai-overview#instructlab-and-rhel-ai[For more detailed information, see the "InstructLab and RHEL AI" documentation section.]

Red Hat Enterprise Linux AI allows you to do the following:

 * Serve an LLM and interact with the open source Granite family of Large Language Models (LLMs) locally or via an exposed endpoint.
 * Interact via chat with the model that has been fine tuned with your data.
 * store data within a taxonomy format to organize knowledge and skills
 * generate synthetic data to utilize in AI model alignment / fine tuning the model with new knowledge or skills.
 * fine-tune a model with that data with minimal machine learning background.


Red Hat Enterprise Linux AI empowers you to contribute directly to LLMs. This allows you to easily and efficiently build AI-based applications, including chatbots.

== Introduction RHEL AI Course

If you are new to RHEL AI, this ~15 minute intermediate course provides a detailed overview of the features, terms, and high level concepts of RHEL AI. We will not review the basics in this course.

https://training-lms.redhat.com/sso/saml/auth/rhlpint?RelayState=deeplinkoffering%3D66356584[Introduction to RHEL AI, window=blank], just launched for technical sellers to get a high-level technical look at RHEL AI and InstructLab

'''

==  RHEL AI on AWS

This course will focus on the knowledge around the benefits of RHEL AI, and the necessary information for making informed decisions on recommending for running RHEL AI on AWS to customers.

Currently there is no managed service offering of RHEL AI on AWS, however RHEL AI does support self-managed deployment on AWS.  While this course mentions RHEL AI, actual deployment of RHEL AI on AWS was released in a standalone course. 

=== Deploying RHEL AI on AWS

Get hands-on experience deploying RHEL AI on AWS. This quick course provides a detailed guide for deploying RHEL AI on AWS.

https://redhatquickcourses.github.io/rhel-ai-aws/rhel-ai-aws/1/index.html[Deploying RHEL AI on AWS, window=blank]

The overall objectives of the Deploying RHEL AI on AWS includes:

* Create and configure a custom RHEL AI AMI image.
* Upload the RHEL AI AMI to an AWS S3 bucket for storage and retrieval.
* Provision AWS resources and deploy a fully functional RHEL AI instance.

To reduce cost this lab exercise includes a basic GPU to run RHEL AI, however performance is limited in this configuration.

[NOTE]
This lab is available in the LMS, so if you complete the lab please launch the course in the LMS as well earn the achievement for completing the course and provide support for the content creator.

== RHEL AI Deployment Approaches on AWS


There are two approaches to utilizing RHEL AI in AWS, 

 . Using RHEL AI as a solution for model inferencing (serving), which means to make the model available via an endpoint for consumption via business applications. 

 . The second option is use RHEL AI to customize, fine-tune, or align the model to specific business knowledge in order to provide more accurate responses to the connecting application.

A single RHEL AI server can perform both functions but it's not recommended to augment (train) a model while also providing inferencing for an application on a single RHEL AI server as performing both services may exhaust accelerator (GPU) memory resources.

Running multiple instances of RHEL AI, one for training and another inferencing can be the most cost effective option as the inferencing requirements are significantly less than the model training specifications.

As you will notice, the requirements for full end-to-end workflow are significantly higher than for model inference.  


=== RHEL AI Instance Options

The following charts show the recommended hardware requirements for running the full InstructLab end-to-end workflow to customize the Granite student model. This includes: synthetic data generation (SDG), training, and evaluating a custom Granite model.

*Amazon Web Service Instance Types*
|===
| Hardware vendor | Supported Accelerators(GPUs) | Memory | AWS Instances | Recommended disk storage
 
| Nvidia
| 8xA100
| 320 GB
| p4d.24xlarge
| 3 TB
 
| Nvidia
| 8xH100
| 640 GB
| p4de.24xlarge
| 3 TB

| Nvidia
| 8xH100
| 640 GB
| p5.48xlarge
| 3 TB

| Nvidia
| 8xL40S
| 384 GB
| g6e.48xlarge
| 3 TB

 
|===

The following charts display the minimum hardware requirements for inference serving a model on Red Hat Enterprise Linux AI.  

*Amazon Web Service Instance Types*
|===
| Hardware vendor | Supported Accelerators(GPUs) | Memory | AWS Instances | Recommended disk storage
 
| Nvidia
| A100
| 40 GB
| P4d series
| 1 TB
 
| Nvidia
| H100
| 80 GB
| P5 series
| 1 TB

| Nvidia
| L40S
| 48 GB
| G6e series
| 1 TB

| Nvidia
| L4
| 24 GB
| G6 series
| 1 TB

|===

Selecting instance types can be calculated based on the size of the model that will be loaded into memory.  A good rule of thumb is to double the number of parameters. For example, a 13B parameter model is recommended to have at least 26GB of GPU memory available.  In this case the 40GB A100 GPU would be a good choice.


'''

Summary

For this course, the topics covered are specifically to understand that you can deploy multiple RHEL AI servers on AWS for both training jobs, and for inference purposes to power applications.

Training instances can be paused or terminated once the model is aligned to specific business cases, saving resources cost for only when model updates, or new model development tasks are needed.

Considering the knowledge learned so far, our FictionCorp use case would benefit from starting with a single RHEL AI instance powering the AI solution. This would be the cost effective approach to running the AI model needed to power the chatbot application.

While OpenShift AI Cloud Services would provide a full service environment, the overhead platform is more than needed for the first phase of project chatbot.

